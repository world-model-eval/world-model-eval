{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58519c8-538f-46e6-a852-55a2fac63d20",
   "metadata": {},
   "source": [
    "# World Model Policy Evaluation\n",
    "\n",
    "This notebook demonstrates how to rollout OpenVLA in the Bridge (WidowX) environment from within our world model, and how to grade it using a VLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e0df8e-d0ec-4978-aa09-5881505d0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758bf78e-730b-49f7-a9bf-a2bf0b456ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "import mediapy as media\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import einops\n",
    "import base64\n",
    "import math\n",
    "import re\n",
    "import cv2\n",
    "from openai import OpenAI\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec9c7c-a162-41bc-b6fb-7b623eff84b9",
   "metadata": {},
   "source": [
    "## Action scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b556ff-254b-48a6-b4e5-d202d11d068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_bridge_action(a):\n",
    "    \"\"\"\n",
    "    Rescale Bridge (WidowX) action to the ranges expected by the world model.\n",
    "    We need to call this function on the unnormalized action values returned by the policy.\n",
    "    \"\"\"\n",
    "    wv_lo = -0.05\n",
    "    wv_hi = +0.05\n",
    "    wv_post_scale_max = +1.75\n",
    "    wv_post_scale_min = -1.75\n",
    "    rd_lo = -0.25\n",
    "    rd_hi = +0.25\n",
    "    rd_post_scale_max = +1.4\n",
    "    rd_post_scale_min = -1.4\n",
    "    # rescale end effector\n",
    "    a[:3] = (a[:3] - wv_lo) / (wv_hi - wv_lo) * (\n",
    "        wv_post_scale_max - wv_post_scale_min\n",
    "    ) + wv_post_scale_min\n",
    "    # rescale joint rotations\n",
    "    a[3:6] = (a[3:6] - rd_lo) / (rd_hi - rd_lo) * (\n",
    "        rd_post_scale_max - rd_post_scale_min\n",
    "    ) + rd_post_scale_min\n",
    "    # threshold the gripper\n",
    "    a[6] = torch.where(a[6] > 0.8, -1.0, +1.0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba762c-34fb-49bc-84ec-019b167200b8",
   "metadata": {},
   "source": [
    "## VLM evaluation utils\n",
    "\n",
    "Set your OpenAI key:\n",
    "```\n",
    "export OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc48ccff-1ed0-4748-9e79-a3d4a9b2c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_video(video, stride=20):\n",
    "    frames, idx = [], 0\n",
    "    for idx, frame in enumerate(video):\n",
    "        if idx % stride == 0:\n",
    "            if (frame == 0).all():\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            _, buf = cv2.imencode(\".jpg\", frame)\n",
    "            frames.append(base64.b64encode(buf).decode())\n",
    "    return frames\n",
    "\n",
    "\n",
    "def predict(video, task, n=5):\n",
    "    frames = encode_video(video)\n",
    "    prompt = f\"\"\"\n",
    "Here is a sequence of frames from a robot policy which has been rolled out in a video-generation-based world model. I need your help determining whether the policy is successful. How successfully does the robot complete the following task?\n",
    "Task: {task[\"instruction\"]}\n",
    "Subtasks: {task[\"subtasks\"]}\n",
    "\n",
    "For each subtask, give the model a score of 0 (fail) or 1 (success). Explain your reasoning. Finally, output \"Total Score: <score>\", where <score> is the sum of the scores on the subtasks above.\n",
    "\n",
    "Note: Since this video was generated by a video prediction model (conditioned on robot actions), it may contain some artifacts due to the video model capacity.\n",
    "Note: For the final score output, make sure to follow the \"Total Score: <score>\" format exactly so the score can be parsed out using a regex.\n",
    "\"\"\".strip()\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [prompt, *[{\"image\": f, \"resize\": 256} for f in frames]],\n",
    "        }\n",
    "    ]\n",
    "    print(prompt)\n",
    "    res = client.chat.completions.create(model=\"gpt-4o\", messages=messages, n=n)\n",
    "    total_score = 0\n",
    "    n_scores = 0\n",
    "    for c in res.choices:\n",
    "        print(c.message.content)\n",
    "        m = re.search(r\"Total Score: (\\d+)\", c.message.content.replace(\"**\", \"\"))\n",
    "        if m:\n",
    "            total_score += (\n",
    "                int(m.group(1)) / 2.0\n",
    "            )  # divide by 2 since there are 2 subtasks\n",
    "            n_scores += 1\n",
    "    avg_score = total_score / n_scores\n",
    "    print(f\"Average Score: {avg_score}\")\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e18c6-ee66-4b1d-9931-a99f52568291",
   "metadata": {},
   "source": [
    "## Policy Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9ae9cec-11b5-4bf2-9cac-3c8d38e17dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(wm, vla, tasks, rollout_length=40):\n",
    "    \"\"\"\n",
    "    Rollout an OpenVLA model on a list of tasks, and return the score on each task.\n",
    "    Arguments:\n",
    "        wm: WorldModel\n",
    "        vla: An OpenVLA model from `transformers`\n",
    "        tasks: A list of N tasks in loaded from a json. See \"put_carrot_on_plate.json\" for an example of the format.\n",
    "    Returns:\n",
    "        scores: A list of N scores from the VLM corresponding to each input task.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for task_i in tqdm(tasks, desc=\"completing tasks\"):\n",
    "        start_frame = np.array(\n",
    "            Image.open(requests.get(task_i[\"im_0_path\"], stream=True).raw).resize(\n",
    "                (256, 256)\n",
    "            )\n",
    "        )\n",
    "        media.show_image(start_frame)\n",
    "        wm.reset(torch.from_numpy(start_frame).cuda().float() / 255.0)\n",
    "\n",
    "        frames = [start_frame]\n",
    "        for step in tqdm(range(rollout_length)):\n",
    "            curr_frame = Image.fromarray(frames[-1])\n",
    "            prompt = f\"In: What action should the robot take to {task_i['instruction']}?\\nOut:\"\n",
    "            inputs = processor(prompt, curr_frame).to(\n",
    "                device=\"cuda\", dtype=torch.bfloat16\n",
    "            )\n",
    "            actions = vla.predict_action(\n",
    "                **inputs, unnorm_key=\"bridge_orig\", do_sample=False\n",
    "            )\n",
    "\n",
    "            a = torch.tensor(actions)[0].cuda()\n",
    "            # NOTE: OpenVLA outputs 7-dim actions, while the world model was trained with up to 10-dim actions.\n",
    "            a = torch.cat([a, a.new_zeros(3)], dim=-1)  # pad with zeros\n",
    "            a = rescale_bridge_action(a)\n",
    "\n",
    "            for i, x in wm.generate_chunk(a):\n",
    "                new_frame = x[0, 0].cpu().numpy()\n",
    "                new_frame = np.clip(new_frame * 255, 0, 255).astype(np.uint8)\n",
    "                frames.append(new_frame)\n",
    "        rollout_video = np.stack(frames)\n",
    "        media.show_video(rollout_video, fps=20)\n",
    "        avg_score = predict(rollout_video, task=task_i)\n",
    "        scores.append(avg_score)\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3a7c2-94e9-4588-b17f-30c1c05453de",
   "metadata": {},
   "source": [
    "## Init world model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29d42bbc-2027-4340-9ad9-c9a9967821d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from world_model import WorldModel\n",
    "import os\n",
    "\n",
    "CHECKPOINTS_TO_KWARGS = {\n",
    "    \"bridge_v2_ckpt.pt\": {  # The demo model checkpoint from our original arxiv release.\n",
    "        \"use_pixel_rope\": True,\n",
    "    },\n",
    "    \"200k_20frame_cfg_bridgev2_ckpt.pt\": {  # New in-progress model with CFG and EMA.\n",
    "        \"use_pixel_rope\": False,\n",
    "        \"default_cfg\": 3.0,\n",
    "    },\n",
    "}\n",
    "FILESERVER_URL = \"https://85daf289d906.ngrok.app\"  # This might change.\n",
    "\n",
    "ckpt_path = \"200k_20frame_cfg_bridgev2_ckpt.pt\"  # Take your pick from above.\n",
    "if not Path(ckpt_path).exists():\n",
    "    ckpt_url = FILESERVER_URL + \"/\" + ckpt_path\n",
    "    print(f\"{ckpt_url=}\")\n",
    "    os.system(f\"wget {ckpt_url}\")\n",
    "\n",
    "wm = WorldModel(ckpt_path, **CHECKPOINTS_TO_KWARGS[ckpt_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489a0e5-ac45-4215-b44a-a4d51284f0e6",
   "metadata": {},
   "source": [
    "## Init OpenVLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c1f486f-79f7-4ee1-9cb9-86596e9d0344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.53.0` and `tokenizers==0.21.4`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a59fd2-81ca-4b8e-8c43-f52450d3f378",
   "metadata": {},
   "source": [
    "## Scrape jpgs from the Bridge website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33f4a8-dfab-4494-9b0c-e6716e647285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def find_jpgs_http(url, session=None):\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    resp = session.get(url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if not href or href.startswith('?') or href.startswith('/'):\n",
    "            continue\n",
    "\n",
    "        full_url = urljoin(url, href)\n",
    "        if href.endswith('/'):\n",
    "            yield from find_jpgs_http(full_url, session=session)\n",
    "        elif href.lower().endswith('im_0.jpg'):\n",
    "            yield full_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e9fe5-8a4c-4b2d-b6b3-ac01fccaa2a0",
   "metadata": {},
   "source": [
    "To choose a different task, edit the `base_url`, `instruction`, and `subtasks` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0cc25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = {\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v1/berkeley/toysink2_bww/put_carrot_on_plate/\": {\n",
    "        \"instruction\": \"put carrot on plate\",\n",
    "        \"subtasks\": [\"Pick up the carrot.\", \"Place the carrot on the plate.\"],\n",
    "    },\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v1/berkeley/toysink2_bww/put_eggplant_into_pot_or_pan/\": {\n",
    "        \"instruction\": \"put eggplant into pot or pan\",\n",
    "        \"subtasks\": [\"Pick up the eggplant.\", \"Place the eggplant into the pot or pan.\"],\n",
    "    },\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v2/deepthought_toykitchen1/close_microwave/\": {\n",
    "        \"instruction\": \"close microwave\",\n",
    "        \"subtasks\": [\"Push the microwave door.\", \"Close the microwave door completely.\"],\n",
    "    },\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v2/deepthought_toykitchen2/stack_blocks/\": {\n",
    "        \"instruction\": \"stack blocks\",\n",
    "        \"subtasks\": [\"Pick up a block.\", \"Place a block on top of another block.\"],\n",
    "    },\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v1/berkeley/toykitchen2/close_oven/\": {\n",
    "        \"instruction\": \"close oven\",\n",
    "        \"subtasks\": [\"Push the oven door.\", \"Close the oven door completely.\"],\n",
    "    },\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v2/deepthought_toykitchen1/open_microwave/\": {\n",
    "        \"instruction\": \"open the microwave\",\n",
    "        \"subtasks\": [\"Grab microwave handle.\", \"Pull microwave open.\"],\n",
    "    },\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v2/datacol2_folding_table_white_tray/sweep_granular/\": {\n",
    "        \"instruction\": \"sweep up the beans\",\n",
    "        \"subtasks\": [\"Grab the metal beam.\", \"Use the metal beam to sweep the beans.\"],\n",
    "    },\n",
    "    \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v2/datacol1_toykitchen6/fold_cloth/\": {\n",
    "        \"instruction\": \"fold cloth\",\n",
    "        \"subtasks\": [\"Pick up one end of the cloth.\", \"Fold the cloth over.\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a5f99ce-4439-486d-9d50-d68ebe2c1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "tasks = []\n",
    "base_url = \"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v1/berkeley/toysink2_bww/put_carrot_on_plate/\"\n",
    "for jpg_url in find_jpgs_http(base_url):\n",
    "    tasks.append(\n",
    "        {\n",
    "            \"im_0_path\": jpg_url,\n",
    "            \"instruction\": TASKS[base_url][\"instruction\"],\n",
    "            \"subtasks\": TASKS[base_url][\"subtasks\"],\n",
    "        }\n",
    "    )\n",
    "tasks = random.sample(tasks, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98a047-522e-4993-8e71-0a463f8e89fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = evaluate_policy(wm, vla, tasks)\n",
    "print(f\"Example task: {tasks[0]}\")\n",
    "print(f\"Mean score: {np.mean(scores)=}\")\n",
    "print(f\"STE: {np.std(scores) / len(scores)**0.5=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "world-model-eval (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
