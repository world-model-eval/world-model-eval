{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58519c8-538f-46e6-a852-55a2fac63d20",
   "metadata": {},
   "source": [
    "# World Model Policy Evaluation\n",
    "\n",
    "This notebook demonstrates how to rollout OpenVLA in the Bridge (WidowX) environment from within our world model, and how to grade it using a VLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0df8e-d0ec-4978-aa09-5881505d0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758bf78e-730b-49f7-a9bf-a2bf0b456ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "import mediapy as media\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import einops\n",
    "import base64\n",
    "import math\n",
    "import re\n",
    "import cv2\n",
    "from openai import OpenAI\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec9c7c-a162-41bc-b6fb-7b623eff84b9",
   "metadata": {},
   "source": [
    "## Action scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b556ff-254b-48a6-b4e5-d202d11d068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_bridge_action(\n",
    "    a,\n",
    "    wv_lo=-0.05,\n",
    "    wv_hi=+0.05,\n",
    "    wv_post_scale_max=+1.75,\n",
    "    wv_post_scale_min=-1.75,\n",
    "    rd_lo=-0.25,\n",
    "    rd_hi=+0.25,\n",
    "    rd_post_scale_max=+1.4,\n",
    "    rd_post_scale_min=-1.4):\n",
    "    \"\"\"\n",
    "    Rescale Bridge (WidowX) action to the ranges expected by the world model.\n",
    "    We need to call this function on the unnormalized action values returned by the policy.\n",
    "    \"\"\"\n",
    "    # rescale end effector\n",
    "    a[:3] = (a[:3] - wv_lo) / (wv_hi - wv_lo) * (\n",
    "        wv_post_scale_max - wv_post_scale_min\n",
    "    ) + wv_post_scale_min\n",
    "    # rescale joint rotations\n",
    "    a[3:6] = (a[3:6] - rd_lo) / (rd_hi - rd_lo) * (\n",
    "        rd_post_scale_max - rd_post_scale_min\n",
    "    ) + rd_post_scale_min\n",
    "    # threshold the gripper\n",
    "    a[6] = torch.where(a[6] > 0.8, -1.0, +1.0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba762c-34fb-49bc-84ec-019b167200b8",
   "metadata": {},
   "source": [
    "## VLM evaluation utils\n",
    "\n",
    "Set your OpenAI key:\n",
    "```\n",
    "export OPENAI_API_KEY=<your key here>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48ccff-1ed0-4748-9e79-a3d4a9b2c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_video(video, stride=20):\n",
    "    frames, idx = [], 0\n",
    "    for idx, frame in enumerate(video):\n",
    "        if idx % stride == 0:\n",
    "            if (frame == 0).all():\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            _, buf = cv2.imencode(\".jpg\", frame)\n",
    "            frames.append(base64.b64encode(buf).decode())\n",
    "    return frames\n",
    "\n",
    "\n",
    "def predict(video, task, n=5):\n",
    "    frames = encode_video(video)\n",
    "    prompt = f\"\"\"\n",
    "Here is a sequence of frames from a robot policy which has been rolled out in a video-generation-based world model. I need your help determining whether the policy is successful. How successfully does the robot complete the following task?\n",
    "Task: {task[\"instruction\"]}\n",
    "Subtasks: {task[\"subtasks\"]}\n",
    "\n",
    "For each subtask, give the model a score of 0 (fail) or 1 (success). Explain your reasoning. Finally, output \"Total Score: <score>\", where <score> is the sum of the scores on the subtasks above.\n",
    "\n",
    "Note: Since this video was generated by a video prediction model (conditioned on robot actions), it may contain some artifacts due to the video model capacity.\n",
    "Note: For the final score output, make sure to follow the \"Total Score: <score>\" format exactly so the score can be parsed out using a regex.\n",
    "\"\"\".strip()\n",
    "    client = OpenAI()\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [prompt, *[{\"image\": f, \"resize\": 256} for f in frames]],\n",
    "        }\n",
    "    ]\n",
    "    print(prompt)\n",
    "    res = client.chat.completions.create(model=\"gpt-4o\", messages=messages, n=n)\n",
    "    total_score = 0\n",
    "    n_scores = 0\n",
    "    for c in res.choices:\n",
    "        print(c.message.content)\n",
    "        m = re.search(r\"Total Score: (\\d+)\", c.message.content.replace(\"**\", \"\"))\n",
    "        if m:\n",
    "            total_score += (\n",
    "                int(m.group(1)) / 2.0\n",
    "            )  # divide by 2 since there are 2 subtasks\n",
    "            n_scores += 1\n",
    "    avg_score = total_score / n_scores\n",
    "    print(f\"Average Score: {avg_score}\")\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3a7c2-94e9-4588-b17f-30c1c05453de",
   "metadata": {},
   "source": [
    "## Init world model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d42bbc-2027-4340-9ad9-c9a9967821d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from world_model import WorldModel\n",
    "import os\n",
    "\n",
    "CHECKPOINTS_TO_KWARGS = {\n",
    "    \"bridge_v2_ckpt.pt\": {  # The demo model checkpoint from our original arxiv release.\n",
    "        \"use_pixel_rope\": True,\n",
    "    },\n",
    "    \"200k_20frame_cfg_bridgev2_ckpt.pt\": {  # New in-progress model with CFG and EMA.\n",
    "        \"use_pixel_rope\": False,\n",
    "        \"default_cfg\": 3.0,\n",
    "    },\n",
    "}\n",
    "FILESERVER_URL = \"https://85daf289d906.ngrok.app\"  # This might change.\n",
    "\n",
    "ckpt_path = \"200k_20frame_cfg_bridgev2_ckpt.pt\"  # Take your pick from above.\n",
    "if not Path(ckpt_path).exists():\n",
    "    ckpt_url = FILESERVER_URL + \"/\" + ckpt_path\n",
    "    print(f\"{ckpt_url=}\")\n",
    "    os.system(f\"wget {ckpt_url}\")\n",
    "\n",
    "wm = WorldModel(ckpt_path, **CHECKPOINTS_TO_KWARGS[ckpt_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6177b00-fef5-4a32-bddf-27c0b826424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_based_model import StateDynamicsModel\n",
    "\n",
    "sm = StateDynamicsModel(\n",
    "    state_dim=7,\n",
    "    action_dim=10,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=12,\n",
    "    frame_skip=1,\n",
    "    ckpt=\"/scratch/as20482/bridge_v2_state_based_1frame.pt\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a59fd2-81ca-4b8e-8c43-f52450d3f378",
   "metadata": {},
   "source": [
    "## Load the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33f4a8-dfab-4494-9b0c-e6716e647285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_tasks(root):\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".png\"):\n",
    "            base = os.path.splitext(file)[0]\n",
    "            yield os.path.join(root, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = {\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/put_carrot_on_plate/\": {\n",
    "        \"instruction\": \"put carrot on plate\",\n",
    "        \"subtasks\": [\"Pick up the carrot.\", \"Place the carrot on the plate.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/put_eggplant_into_pot_or_pan/\": {\n",
    "        \"instruction\": \"put eggplant into pot or pan\",\n",
    "        \"subtasks\": [\"Pick up the eggplant.\", \"Place the eggplant into the pot or pan.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/close_microwave/\": {\n",
    "        \"instruction\": \"close microwave\",\n",
    "        \"subtasks\": [\"Push the microwave door.\", \"Close the microwave door completely.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/stack_blocks/\": {\n",
    "        \"instruction\": \"stack blocks\",\n",
    "        \"subtasks\": [\"Pick up a block.\", \"Place a block on top of another block.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/close_oven/\": {\n",
    "        \"instruction\": \"close oven\",\n",
    "        \"subtasks\": [\"Push the oven door.\", \"Close the oven door completely.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/open_microwave/\": {\n",
    "        \"instruction\": \"open the microwave\",\n",
    "        \"subtasks\": [\"Grab microwave handle.\", \"Pull microwave open.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/sweep_into_pile/\": {\n",
    "        \"instruction\": \"sweep into pile\",\n",
    "        \"subtasks\": [\"Grab the beam.\", \"Use the beam to sweep into pile.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/fold_cloth/\": {\n",
    "        \"instruction\": \"fold cloth\",\n",
    "        \"subtasks\": [\"Pick up one end of the cloth.\", \"Fold the cloth over.\"],\n",
    "    },\n",
    "    \"/vast/as20482/data/bridge/robot_evaluation/open_oven/\": {\n",
    "        \"instruction\": \"open oven\",\n",
    "        \"subtasks\": [\"Grab oven handle.\", \"Pull oven open.\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f99ce-4439-486d-9d50-d68ebe2c1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "base_path = \"/vast/as20482/data/bridge/robot_evaluation/open_oven/\"\n",
    "for task in load_tasks(base_path):\n",
    "    tasks.append(\n",
    "        {\n",
    "            \"im_0_path\": task,\n",
    "            \"instruction\": TASKS[base_path][\"instruction\"],\n",
    "            \"subtasks\": TASKS[base_path][\"subtasks\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e18c6-ee66-4b1d-9931-a99f52568291",
   "metadata": {},
   "source": [
    "## OpenVLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae9cec-11b5-4bf2-9cac-3c8d38e17dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_openvla(wm, vla, tasks, rollout_length=40):\n",
    "    \"\"\"\n",
    "    Rollout an OpenVLA model on a list of tasks, and return the score on each task.\n",
    "    Arguments:\n",
    "        wm: WorldModel\n",
    "        vla: An OpenVLA model from `transformers`\n",
    "        tasks: A list of N tasks in loaded from a json. See \"put_carrot_on_plate.json\" for an example of the format.\n",
    "    Returns:\n",
    "        scores: A list of N scores from the VLM corresponding to each input task.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for task_i in tqdm(tasks, desc=\"completing tasks\"):\n",
    "        start_frame = np.array(\n",
    "            Image.open(task_i[\"im_0_path\"]+\".png\").resize(\n",
    "                (256, 256)\n",
    "            )\n",
    "        )\n",
    "        media.show_image(start_frame)\n",
    "        wm.reset(torch.from_numpy(start_frame).cuda().float() / 255.0)\n",
    "\n",
    "        frames = [start_frame]\n",
    "        for step in tqdm(range(rollout_length)):\n",
    "            curr_frame = Image.fromarray(frames[-1])\n",
    "            prompt = f\"In: What action should the robot take to {task_i['instruction']}?\\nOut:\"\n",
    "            inputs = processor(prompt, curr_frame).to(\n",
    "                device=\"cuda\", dtype=torch.bfloat16\n",
    "            )\n",
    "            actions = vla.predict_action(\n",
    "                **inputs, unnorm_key=\"bridge_orig\", do_sample=False\n",
    "            )\n",
    "\n",
    "            a = torch.tensor(actions)[0].cuda()\n",
    "            # NOTE: OpenVLA outputs 7-dim actions, while the world model was trained with up to 10-dim actions.\n",
    "            a = torch.cat([a, a.new_zeros(3)], dim=-1)  # pad with zeros\n",
    "            a = rescale_bridge_action(a)\n",
    "\n",
    "            for i, x in wm.generate_chunk(a):\n",
    "                new_frame = x[0, 0].cpu().numpy()\n",
    "                new_frame = np.clip(new_frame * 255, 0, 255).astype(np.uint8)\n",
    "                frames.append(new_frame)\n",
    "        rollout_video = np.stack(frames)\n",
    "        media.show_video(rollout_video, fps=20)\n",
    "        avg_score = predict(rollout_video, task=task_i)\n",
    "        scores.append(avg_score)\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f486f-79f7-4ee1-9cb9-86596e9d0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98a047-522e-4993-8e71-0a463f8e89fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = evaluate_openvla(wm, vla, tasks)\n",
    "print(f\"Example task: {tasks[0]}\")\n",
    "print(f\"Mean score: {np.mean(scores)=}\")\n",
    "print(f\"STE: {np.std(scores) / len(scores)**0.5=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527149d5-085c-4399-92d6-9a3ed61a4b09",
   "metadata": {},
   "source": [
    "## HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033f0de-6a76-463d-a3c9-1e293faeff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpt.models.policy import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00105ca-b670-41ce-8534-7933f30762a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy.from_pretrained_full_model(\"hpt-large-lang\", \"bridge\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91441cbe-68d4-41c2-b6a6-8e1c49ebcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hpt(wm, sm, vla, tasks, rollout_length=40):\n",
    "    scores = []\n",
    "    for task_i in tqdm(tasks, desc=\"completing tasks\"):\n",
    "        start_frame = np.array(\n",
    "            Image.open(task_i[\"im_0_path\"]+\".png\").resize(\n",
    "                (256, 256)\n",
    "            )\n",
    "        )\n",
    "        state = np.load(task_i[\"im_0_path\"]+\".state.npy\")\n",
    "        media.show_image(start_frame)\n",
    "        wm.reset(torch.from_numpy(start_frame).cuda().float() / 255.0)\n",
    "\n",
    "        frames = [start_frame]\n",
    "        vla.reset()\n",
    "        for step in tqdm(range(rollout_length)):\n",
    "            curr_frame = frames[-1]\n",
    "            prompt = f\"In: What action should the robot take to {task_i['instruction']}?\\nOut:\"\n",
    "            inputs = {\n",
    "                'image': curr_frame,\n",
    "                'state': state,\n",
    "                'language': prompt\n",
    "            }\n",
    "            raw_actions = vla.get_action(inputs, domain=\"bridge\")\n",
    "            actions = policy.normalizer[\"bridge\"][\"action\"].normalize(raw_actions)\n",
    "            a = torch.tensor(actions).cuda()\n",
    "            a = torch.cat([a, a.new_zeros(4)], dim=-1)  # pad with zeros\n",
    "            a = rescale_bridge_action(a, wv_lo=-1, wv_hi=1, rd_lo=-1, rd_hi=1)\n",
    "            state = sm(torch.from_numpy(state).unsqueeze(0).cuda(), a.unsqueeze(0).unsqueeze(0)).detach().cpu().numpy()[0][0]\n",
    "\n",
    "            for i, x in wm.generate_chunk(a):\n",
    "                new_frame = x[0, 0].cpu().numpy()\n",
    "                new_frame = np.clip(new_frame * 255, 0, 255).astype(np.uint8)\n",
    "                frames.append(new_frame)\n",
    "        rollout_video = np.stack(frames)\n",
    "        media.show_video(rollout_video, fps=20)\n",
    "        avg_score = predict(rollout_video, task=task_i)\n",
    "        scores.append(avg_score)\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a6533-4b48-457e-893f-d1be03d491f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_hpt(wm, sm, policy, tasks)\n",
    "print(f\"Example task: {tasks[0]}\")\n",
    "print(f\"Mean score: {np.mean(scores)=}\")\n",
    "print(f\"STE: {np.std(scores) / len(scores)**0.5=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e208e-4c85-40ea-bded-6cad06cf322d",
   "metadata": {},
   "source": [
    "## SpatialVLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1e24c-2a1d-4b98-8b02-e13abf2e4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor, AutoModel\n",
    "\n",
    "model_name_or_path=\"IPEC-COMMUNITY/spatialvla-4b-224-pt\"\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9178a21-5394-46dd-87f4-89043f759755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_actions(unnorm_actions, statistics, key=\"bridge_orig/1.0.0\"):\n",
    "    stats = statistics[key][\"action\"]\n",
    "    action_low = np.array(stats[\"q01\"])\n",
    "    action_high = np.array(stats[\"q99\"])\n",
    "    mask = np.array(stats.get(\"mask\", np.ones_like(action_low)), dtype=bool)\n",
    "\n",
    "    norm_actions = np.where(\n",
    "        mask,\n",
    "        2 * (unnorm_actions - action_low) / (action_high - action_low) - 1,\n",
    "        unnorm_actions,  # leave unmasked dimensions as-is\n",
    "    )\n",
    "    return norm_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66f3c5-3e21-41d6-9267-bc94ae2202d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spatialvla(wm, vla, tasks, rollout_length=40):\n",
    "    scores = []\n",
    "    for task_i in tqdm(tasks, desc=\"completing tasks\"):\n",
    "        start_frame = np.array(\n",
    "            Image.open(task_i[\"im_0_path\"]+\".png\").resize(\n",
    "                (256, 256)\n",
    "            )\n",
    "        )\n",
    "        media.show_image(start_frame)\n",
    "        wm.reset(torch.from_numpy(start_frame).cuda().float() / 255.0)\n",
    "\n",
    "        frames = [start_frame]\n",
    "        for step in tqdm(range(rollout_length)):\n",
    "            curr_frame = Image.fromarray(frames[-1])\n",
    "            prompt = f\"In: What action should the robot take to {task_i['instruction']}?\\nOut:\"\n",
    "            inputs = processor(images=[curr_frame], text=prompt, return_tensors=\"pt\")\n",
    "            generation_outputs = model.predict_action(inputs)\n",
    "            unnorm_actions = processor.decode_actions(generation_outputs, unnorm_key=\"bridge_orig/1.0.0\")['actions'][0]\n",
    "            actions = normalize_actions(unnorm_actions, processor.statistics)\n",
    "            a = torch.tensor(actions).cuda()\n",
    "            a = torch.cat([a, a.new_zeros(3)], dim=-1)  # pad with zeros\n",
    "            a = rescale_bridge_action(a, wv_lo=-1, wv_hi=1, rd_lo=-1, rd_hi=1)\n",
    "\n",
    "            for i, x in wm.generate_chunk(a):\n",
    "                new_frame = x[0, 0].cpu().numpy()\n",
    "                new_frame = np.clip(new_frame * 255, 0, 255).astype(np.uint8)\n",
    "                frames.append(new_frame)\n",
    "        rollout_video = np.stack(frames)\n",
    "        media.show_video(rollout_video, fps=20)\n",
    "        avg_score = predict(rollout_video, task=task_i)\n",
    "        scores.append(avg_score)\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4097b-bdb8-42c4-b26b-217b402a09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_spatialvla(wm, model, tasks)\n",
    "print(f\"Example task: {tasks[0]}\")\n",
    "print(f\"Mean score: {np.mean(scores)=}\")\n",
    "print(f\"STE: {np.std(scores) / len(scores)**0.5=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8b820-aca2-4ea9-9b5e-f5d493b7fb86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpt",
   "language": "python",
   "name": "hpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
